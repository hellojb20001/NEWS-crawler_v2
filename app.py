import json
import datetime # app.pyì—ì„œë„ datetimeì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì¶”ê°€ (ì„ íƒ ì‚¬í•­, íŒŒì¼ëª… ìƒì„± ë¡œì§ì´ ì œê±°ë˜ì—ˆìœ¼ë¯€ë¡œ í˜„ì¬ëŠ” ë¶ˆí•„ìš”)
import random
import time
from flask import Flask, render_template, request, jsonify
from news.newsbrief_all import crawl_newspaper_articles # news í´ë” ì•ˆì˜ newsbrief_all.pyì—ì„œ í•¨ìˆ˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
from config import newspaper_groups # config.pyì—ì„œ ì‹ ë¬¸ì‚¬ ê·¸ë£¹ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
# from selenium.webdriver.chrome.service import Service # webdriver_manager ì‚¬ìš© ì‹œ í•„ìš”
# from webdriver_manager.chrome import ChromeDriverManager # webdriver_manager ì‚¬ìš© ì‹œ í•„ìš”
from multiprocessing import Pool, Manager, cpu_count # multiprocessing ì¶”ê°€

app = Flask(__name__)

# --- ì‹ ë¬¸ì‚¬ ê·¸ë£¹ ì •ì˜ ---
all_papers_list = []
seen_oids = set()
for group_key in newspaper_groups:
    if group_key != 'all':
        for name, oid in newspaper_groups[group_key]:
            if oid not in seen_oids:
                all_papers_list.append((name, oid))
                seen_oids.add(oid)
newspaper_groups['all'] = sorted(list(set(all_papers_list)))

def setup_chrome_driver():
    """Chrome WebDriver ì„¤ì •ì„ ìœ„í•œ í•¨ìˆ˜ (ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ í˜¸ì¶œë¨)"""
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--start-maximized")
    # íŠ¹ì • User-Agentê°€ ì°¨ë‹¨ì„ ìœ ë°œí•˜ëŠ” ê²½ìš°ê°€ ì•„ë‹ˆë¼ë©´, Selenium ê¸°ë³¸ User-Agentë‚˜ ë” ì¼ë°˜ì ì¸ User-Agentë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36") # User-Agent ì—…ë°ì´íŠ¸
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    chrome_options.add_argument('--disable-gpu') # ì¼ë¶€ ì‹œìŠ¤í…œì—ì„œ headless ì•ˆì •ì„± ë° ì„±ëŠ¥ í–¥ìƒ
    chrome_options.add_argument('--no-sandbox') # Docker ë˜ëŠ” CI í™˜ê²½ì—ì„œ í•„ìš”í•  ìˆ˜ ìˆìŒ
    chrome_options.add_argument('--disable-dev-shm-usage') # Docker ë˜ëŠ” CI í™˜ê²½ì—ì„œ ë¦¬ì†ŒìŠ¤ ë¶€ì¡± ë¬¸ì œ ë°©ì§€
    chrome_options.add_argument('--log-level=3') # ì½˜ì†” ë¡œê·¸ ë ˆë²¨ ì„¤ì • (ì˜¤ë¥˜ë§Œ í‘œì‹œ)
    chrome_options.add_argument('--blink-settings=imagesEnabled=false') # ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™” (ì†ë„ í–¥ìƒ)

    # WebDriver ê²½ë¡œ ìë™ ê´€ë¦¬ë¥¼ ìœ„í•´ webdriver_manager ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.
    # driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    # ì‹œìŠ¤í…œ PATHì— ChromeDriverê°€ ì„¤ì •ë˜ì–´ ìˆë‹¤ê³  ê°€ì •
    try:
        driver = webdriver.Chrome(options=chrome_options)
    except Exception as e:
        print(f"WebDriver ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # webdriver_manager ì‚¬ìš© ì˜ˆì‹œ (ì£¼ì„ í•´ì œ í›„ ì‚¬ìš©)
        # from selenium.webdriver.chrome.service import Service
        # from webdriver_manager.chrome import ChromeDriverManager
        # print("webdriver_managerë¥¼ ì‚¬ìš©í•˜ì—¬ ChromeDriverë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.")
        # driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
        raise # ì˜¤ë¥˜ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ í˜¸ì¶œí•œ ìª½ì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ í•¨
    return driver

# ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‘ì—…ì í•¨ìˆ˜
def crawl_worker(args):
    """
    ë‹¨ì¼ ì‹ ë¬¸ì‚¬ í¬ë¡¤ë§ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ (ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰).
    ê²°ê³¼ë¡œ (ì‹ ë¬¸ì‚¬ëª…, ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸) ë˜ëŠ” (ì‹ ë¬¸ì‚¬ëª…, ì—ëŸ¬ ì •ë³´)ë¥¼ ë°˜í™˜.
    """
    newspaper_name, oid, crawl_scope, process_id = args
    print(f"[Process-{process_id}] '{newspaper_name}' (OID: {oid}) í¬ë¡¤ë§ ì‹œì‘. ë²”ìœ„: {crawl_scope}")
    driver = None
    try:
        driver = setup_chrome_driver()
        articles_list = crawl_newspaper_articles(driver, newspaper_name, oid, crawl_scope)
        print(f"[Process-{process_id}] '{newspaper_name}' í¬ë¡¤ë§ ì™„ë£Œ. ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜: {len(articles_list)}")
        return (newspaper_name, articles_list)
    except Exception as e:
        print(f"[Process-{process_id}] '{newspaper_name}' í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë‚˜ íŠ¹ì • ì˜¤ë¥˜ ë©”ì‹œì§€ ë°˜í™˜ ê°€ëŠ¥
        return (newspaper_name, [("ì˜¤ë¥˜ ë°œìƒ: " + str(e), "")])
    finally:
        if driver:
            print(f"[Process-{process_id}] '{newspaper_name}' WebDriver ì¢…ë£Œ.")
            driver.quit()

@app.route('/')
def index():
    """ë©”ì¸ í˜ì´ì§€ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤."""
    return render_template('index.html', newspaper_groups=newspaper_groups)

@app.route('/crawl', methods=['POST'])
def crawl():
    """ì„ íƒëœ ì‹ ë¬¸ì‚¬ì˜ ê¸°ì‚¬ë¥¼ ë³‘ë ¬ë¡œ í¬ë¡¤ë§í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    selected_newspapers_json = request.form.get('selected_newspapers', '[]')
    try:
        selected_newspapers = json.loads(selected_newspapers_json)
    except json.JSONDecodeError:
        return jsonify({'error': 'ì˜ëª»ëœ ì‹ ë¬¸ì‚¬ ì„ íƒ í˜•ì‹ì…ë‹ˆë‹¤.'}), 400
        
    crawl_scope = request.form.get('scope', 'ì „ì²´')
    
    if not selected_newspapers:
        return jsonify({'error': 'ì‹ ë¬¸ì‚¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.'}), 400
    
    newspaper_oid_map = {}
    for group_list in newspaper_groups.values():
        for name, oid_val in group_list: # ë³€ìˆ˜ëª… oidê°€ í•¨ìˆ˜ ì¸ì oidì™€ ê²¹ì¹˜ì§€ ì•Šë„ë¡ oid_valë¡œ ë³€ê²½
            newspaper_oid_map[name] = oid_val
    
    all_articles_from_workers = {}
    
    # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‘ì—… ì¸ì ì¤€ë¹„
    tasks = []
    for i, newspaper_name in enumerate(selected_newspapers):
        if newspaper_name in newspaper_oid_map:
            oid = newspaper_oid_map[newspaper_name]
            tasks.append((newspaper_name, oid, crawl_scope, i + 1)) # ê° ì‘ì—…ì— ê³ ìœ  ID (process_id) ë¶€ì—¬
        else:
            print(f"ê²½ê³ : '{newspaper_name}'ì— ëŒ€í•œ OIDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            all_articles_from_workers[newspaper_name] = [("OIDë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ì‹ ë¬¸ì‚¬ì…ë‹ˆë‹¤.", "")]

    if not tasks: # OIDë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ì‹ ë¬¸ì‚¬ë§Œ ì„ íƒëœ ê²½ìš°
        if all_articles_from_workers: # OID ëª»ì°¾ì€ ì‹ ë¬¸ì‚¬ ì •ë³´ë¼ë„ ë°˜í™˜
             return jsonify({
                'success': True,
                'articles': all_articles_from_workers
            })
        return jsonify({'error': 'í¬ë¡¤ë§í•  ìœ íš¨í•œ ì‹ ë¬¸ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.'}), 400

    # CPU ì½”ì–´ ìˆ˜ë¥¼ ê³ ë ¤í•˜ì—¬ ì ì ˆí•œ í”„ë¡œì„¸ìŠ¤ ìˆ˜ ì„¤ì • (ë„ˆë¬´ ë§ìœ¼ë©´ ì˜¤íˆë ¤ ì„±ëŠ¥ ì €í•˜ ë° ì„œë²„ ë¶€í•˜)
    # ì‹œìŠ¤í…œ í™˜ê²½ì— ë”°ë¼ ì ì ˆí•œ ê°’ì„ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ìµœëŒ€ 4ê°œ ë˜ëŠ” CPU ì½”ì–´ ìˆ˜ ì¤‘ ì‘ì€ ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    num_processes = min(max(1, cpu_count() // 2), 4) # ìµœì†Œ 1ê°œ, ìµœëŒ€ 4ê°œ ë˜ëŠ” CPU ì½”ì–´ì˜ ì ˆë°˜
    if len(tasks) < num_processes: # ì‘ì—… ìˆ˜ë³´ë‹¤ í”„ë¡œì„¸ìŠ¤ ìˆ˜ê°€ ë§ì„ í•„ìš” ì—†ìŒ
        num_processes = len(tasks)
    
    print(f"ë³‘ë ¬ í¬ë¡¤ë§ ì‹œì‘: {len(tasks)}ê°œ ì‹ ë¬¸ì‚¬, {num_processes}ê°œ í”„ë¡œì„¸ìŠ¤ ì‚¬ìš© ì˜ˆì •")

    start_time = time.time()
    
    try:
        with Pool(processes=num_processes) as pool:
            # pool.mapì€ ë‹¨ì¼ ì¸ìë§Œ ë°›ìœ¼ë¯€ë¡œ, crawl_workerê°€ ë‹¨ì¼ íŠœí”Œì„ ë°›ë„ë¡ ìˆ˜ì •í–ˆìŒ
            results = pool.map(crawl_worker, tasks)
        
        for newspaper_name_result, articles_list_result in results:
            all_articles_from_workers[newspaper_name_result] = articles_list_result
            
        # --- ìë™ íŒŒì¼ ì €ì¥ ë¡œì§ì€ í˜„ì¬ ë¹„í™œì„±í™” ìƒíƒœ ìœ ì§€ ---
        # print(f"ìë™ íŒŒì¼ ì €ì¥ ë¹„í™œì„±í™”ë¨. ê²°ê³¼ëŠ” ì›¹ í™”ë©´ìœ¼ë¡œ ì „ì†¡ë©ë‹ˆë‹¤.")
        
        end_time = time.time()
        print(f"ì´ í¬ë¡¤ë§ ì†Œìš” ì‹œê°„: {end_time - start_time:.2f} ì´ˆ")

        return jsonify({
            'success': True,
            'articles': all_articles_from_workers
        })
        
    except Exception as e:
        print(f"í¬ë¡¤ë§ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ë¶€ë¶„ì ì¸ ê²°ê³¼ë¼ë„ ìˆìœ¼ë©´ ë°˜í™˜ (ì„ íƒ ì‚¬í•­)
        if all_articles_from_workers:
             print("--- ì˜¤ë¥˜ ë°œìƒ ì‹œì ì˜ ë¶€ë¶„ ê²°ê³¼ (ìµœëŒ€ 2ê°œ í•­ëª©) ---")
             for newspaper, articles_data in all_articles_from_workers.items():
                 print(f"[{newspaper}]")
                 for i, article_tuple in enumerate(articles_data[:2]):
                     print(f"  {i+1}: {article_tuple}")
             print("----------------------------------------------------")
        return jsonify({'error': f'ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜: {str(e)}'}), 500
    # finally ë¸”ë¡ì—ì„œ ë©”ì¸ ë“œë¼ì´ë²„ë¥¼ ì¢…ë£Œí•  í•„ìš”ê°€ ì—†ìŒ (ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ ê´€ë¦¬)

if __name__ == '__main__':
    # ë©€í‹°í”„ë¡œì„¸ì‹± ì‚¬ìš© ì‹œ __main__ ë³´í˜¸ í•„ìˆ˜ (íŠ¹íˆ Windows)
    # Flask ê°œë°œ ì„œë²„ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë””ë²„ê·¸ ëª¨ë“œì—ì„œ reloaderë¥¼ ì‚¬ìš©í•˜ëŠ”ë°,
    # ì´ëŠ” ë©€í‹°í”„ë¡œì„¸ì‹±ê³¼ ì¶©ëŒì„ ì¼ìœ¼í‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” Gunicornê³¼ ê°™ì€ WSGI ì„œë²„ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
    # ê°œë°œ ì¤‘ì—ëŠ” use_reloader=False ì˜µì…˜ì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    app.run(debug=True, use_reloader=False) # use_reloader=False ì¶”ê°€


# ê¸°ì¡´ ì½”ë“œ 

# from flask import Flask, render_template, request, jsonify
# from news.newsbrief_all import crawl_newspaper_articles # news í´ë” ì•ˆì˜ newsbrief_all.pyì—ì„œ í•¨ìˆ˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
# from config import newspaper_groups # config.pyì—ì„œ ì‹ ë¬¸ì‚¬ ê·¸ë£¹ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
# from selenium import webdriver
# from selenium.webdriver.chrome.options import Options
# # from selenium.webdriver.chrome.service import Service # webdriver_manager ì‚¬ìš© ì‹œ í•„ìš”
# # from webdriver_manager.chrome import ChromeDriverManager # webdriver_manager ì‚¬ìš© ì‹œ í•„ìš”
# import datetime
# import random
# import time
# import json

# app = Flask(__name__)

# # --- ì‹ ë¬¸ì‚¬ ê·¸ë£¹ ì •ì˜ ---
# # config.pyì—ì„œ ê°€ì ¸ì˜¨ newspaper_groupsë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
# # 'all' ê·¸ë£¹ì€ í•„ìš”ì‹œ ì—¬ê¸°ì„œ (ì¬)êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# all_papers_list = []
# seen_oids = set()
# for group_key in newspaper_groups:
#     if group_key != 'all': 
#         for name, oid in newspaper_groups[group_key]:
#             if oid not in seen_oids:
#                 all_papers_list.append((name, oid))
#                 seen_oids.add(oid)
# newspaper_groups['all'] = sorted(list(set(all_papers_list)))

# def setup_chrome_driver():
#     """Chrome WebDriver ì„¤ì •ì„ ìœ„í•œ í•¨ìˆ˜"""
#     chrome_options = Options()
#     chrome_options.add_argument("--headless")
#     chrome_options.add_argument("--start-maximized")
#     chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36")
#     chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
#     chrome_options.add_experimental_option('useAutomationExtension', False)
#     chrome_options.add_argument('--disable-blink-features=AutomationControlled')
#     # WebDriver ê²½ë¡œ ìë™ ê´€ë¦¬ë¥¼ ìœ„í•´ webdriver_manager ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.
#     # ì•„ë˜ ì£¼ì„ ì²˜ë¦¬ëœ ì½”ë“œë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜, ì‹œìŠ¤í…œ PATHì— ChromeDriverê°€ ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
#     # driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
#     return webdriver.Chrome(options=chrome_options)

# @app.route('/')
# def index():
#     """ë©”ì¸ í˜ì´ì§€ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤."""
#     return render_template('index.html', newspaper_groups=newspaper_groups)

# @app.route('/crawl', methods=['POST'])
# def crawl():
#     """ì„ íƒëœ ì‹ ë¬¸ì‚¬ì˜ ê¸°ì‚¬ë¥¼ í¬ë¡¤ë§í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
#     selected_newspapers = json.loads(request.form.get('selected_newspapers', '[]'))
#     crawl_scope = request.form.get('scope', 'ì „ì²´')
    
#     if not selected_newspapers:
#         return jsonify({'error': 'ì‹ ë¬¸ì‚¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.'}), 400
    
#     newspaper_oid_map = {}
#     for group_list in newspaper_groups.values():
#         for name, oid in group_list:
#             newspaper_oid_map[name] = oid
    
#     all_articles = {}
#     driver = None
    
#     try:
#         driver = setup_chrome_driver()
#         for newspaper_name in selected_newspapers:
#             if newspaper_name in newspaper_oid_map:
#                 oid = newspaper_oid_map[newspaper_name]
#                 articles_list = crawl_newspaper_articles(driver, newspaper_name, oid, crawl_scope)
#                 all_articles[newspaper_name] = articles_list
#                 time.sleep(random.uniform(1.0, 2.5))
#             else:
#                 print(f"ê²½ê³ : '{newspaper_name}'ì— ëŒ€í•œ OIDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
#                 all_articles[newspaper_name] = [("OIDë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ì‹ ë¬¸ì‚¬ì…ë‹ˆë‹¤.", "")]

#         # --- ìë™ íŒŒì¼ ì €ì¥ ë¡œì§ ì œê±° ---
#         # # Generate filename
#         # now = datetime.datetime.now()
#         # filename = f"ì‹ ë¬¸ê¸°ì‚¬_{now.strftime('%Y%m%d_%H%M%S')}.txt"
        
#         # # Save results to file
#         # with open(filename, "w", encoding="utf-8") as f:
#         #     f.write("ğŸ“° ì˜¤ëŠ˜ì˜ ì‹ ë¬¸ ê¸°ì‚¬ ëª¨ìŒ\n\n")
            
#         #     if not all_articles:
#         #         f.write("ìˆ˜ì§‘ëœ ì‹ ë¬¸ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.\n")
#         #     else:
#         #         for newspaper_name_from_dict, article_tuples in all_articles.items():
#         #             f.write(f"ğŸ“Œ [{newspaper_name_from_dict}]\n")
#         #             if not article_tuples:
#         #                 f.write("  ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.\n")
#         #             else:
#         #                 for idx, (title, link) in enumerate(article_tuples, 1):
#         #                     # ì„œë²„ ì½˜ì†”ì— ë””ë²„ê¹… ë©”ì‹œì§€ ì¶œë ¥ (ì‹¤ì œ ì €ì¥ë˜ëŠ” ë‚´ìš© í™•ì¸ìš©)
#         #                     # print(f"DEBUG: Would save to file - Title: '{title}', Link: '{link}'") # í•„ìš”ì‹œ ì£¼ì„ í•´ì œ
#         #                     f.write(f" ğŸ”¹ {title}\n")
#         #                     if link and str(link).strip():
#         #                         f.write(f"    {str(link).strip()}\n")
#         #                     else:
#         #                         f.write(f"    [ë§í¬ ì •ë³´ ì—†ìŒ]\n")
#         #             f.write("\n")
#         # print(f"ìë™ íŒŒì¼ ì €ì¥ ë¹„í™œì„±í™”ë¨. ê²°ê³¼ëŠ” ì›¹ í™”ë©´ìœ¼ë¡œ ì „ì†¡ë©ë‹ˆë‹¤.")
#         # --- ìë™ íŒŒì¼ ì €ì¥ ë¡œì§ ì œê±° ì™„ë£Œ ---
        
#         # í´ë¼ì´ì–¸íŠ¸(ì›¹ ë¸Œë¼ìš°ì €)ì— JSON í˜•íƒœë¡œ ê²°ê³¼ ë°˜í™˜
#         return jsonify({
#             'success': True,
#             # 'filename': filename, # ìë™ ì €ì¥ íŒŒì¼ëª…ì´ ì—†ìœ¼ë¯€ë¡œ ì´ ë¶€ë¶„ë„ ì£¼ì„ ì²˜ë¦¬í•˜ê±°ë‚˜ ì œê±°
#             'articles': all_articles 
#         })
        
#     except Exception as e:
#         print(f"Error during crawl: {e}")
#         if all_articles:
#             print("--- Error context: all_articles (first 2 items per newspaper) ---")
#             for newspaper, articles_data in all_articles.items():
#                 print(f"[{newspaper}]")
#                 for i, article_tuple in enumerate(articles_data[:2]):
#                     print(f"  {i+1}: {article_tuple}")
#             print("-----------------------------------------------------------------")
#         return jsonify({'error': str(e)}), 500
#     finally:
#         if driver:
#             driver.quit()

# if __name__ == '__main__':
#     app.run(debug=True)
